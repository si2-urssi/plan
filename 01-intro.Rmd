---
bibliography: ["references.bib"]
biblio-style: "apalike"
link-citations: true
---

# Introduction {#intro}

Software pervades all parts of modern scientific research, including data analysis and inference as well as computational science. Recent surveys in the US and UK show that 90-95% of researchers rely on research software, and 63-70% of them cannot continue their work if these software were to stop functioning [@hettrick2014]. Much of this software is developed by researchers for researchers, as the contemporary scientific process demands the development of new methods in tandem with the demands of new discoveries and fields. However, despite its importance, a large proportion of research software is developed in an ad hoc manner, with little regard for the high standards that are characteristic of other research activities. As a result, the research software ecosystem is fragile and the source of numerous problems that plague modern computational science [@carver2018conceptualization].

Researchers today are under intense pressure to demonstrate expertise in their chosen domains while also trying to maintain a working current knowledge of software engineering practices. This combination is unsustainable for most researchers. With little bandwidth to keep up with best practices or sufficient recognition of software development as a scholarly activity, much research software is developed in a manner that makes it wholly unsustainable, despite the obvious role that it plays in modern research, for a multitude of reasons. Academic promotion and tenure, even in institutions with liberal policies, consider peer-reviewed publications to be the primary metric of progress in most disciplines. Even when the impact of software is made clear through citation, it is usually not considered a traditional scholarly activity, making it very challenging to get credit. There is no shortage of horror stories of academics who have built demonstrably impactful software, only to be denied tenure. Even outside the tenure track, only a few academic jobs offer meaningful career progression for software work. A second reason, strongly correlated with the lack of recognition of software, is the lack of training opportunities. Many research software engineers are self taught. Others learn programming from bootcamps and workshops rather than in traditional academic coursework. Overworked academics are unable to take advantage of such opportunities and therefore develop software using outdated practices. Lastly, even when software is recognized as having impact, funding agencies rarely fund maintenance and ongoing development of such work, leading to reinvention rather than reuse ([https://chanzuckerberg.com/rfa/essential-open-source-software-for-science/](https://chanzuckerberg.com/rfa/essential-open-source-software-for-science/)). 

We have spent the last two years engaged in a series of activities designed to gain a deeper understanding of why research software is so unsustainable and what can be done about it. Through numerous discussions with diverse groups of researchers, we have brainstormed challenges and solutions that can meaningfully impact the largest number of researchers. This report describes those activities (in [Section 2](#section2)) and outlines plans (in [Section 3](#section3)) for a new institute that will work in multiple areas to improve research software and the careers of those that produce it, with an end goal of performing better research. 

## Nature of the problem

One would be hard pressed to name any field of scientific endeavor that has not been substantially transformed by software. From physics to psychology, software has transformed the way we create, acquire, process, model, and draw insights from data. Much of this transformation has come from the increasing availability of open source tools, many of which have helped improve the rigor, quality, and reproducibility of research. The development of research software is often not considered scholarship, making it very difficult for academics to seek funding and find meaningful career paths, especially when research software activities make up a significant part of their contributions. Such people often lead double lives, working tirelessly to meet the traditional responsibilities of academic life, while developing open source tools that enable modern research. We are able to break the problem down into the following four areas:

**Research software itself is not sustainably developed:** In many fields research software is developed by academics for other academics. Because these people have spent much of their careers developing deep domain expertise, but not on developing software, software doesn’t get the same level of care ([https://www.nature.com/articles/s41592-019-0686-2](https://www.nature.com/articles/s41592-019-0686-2)). Therefore the quality of the software is highly variable, making it hard to sustain. Mounting technical debt often makes it easier to develop software from scratch than to use existing tools. Versions of software used in papers are exceedingly hard to track down, making it challenging to reproduce research findings or reuse research software. When Collberg and colleagues [@collberg2014measuring ;@collberg2015repeatability] decided to measure the extent of the problem precisely, they investigated the availability of code and data as well as the extent to which this code would actually build with reasonable effort. The results were dramatic: of the 515 (out of 613) potentially reproducible papers, across applied computational research, the authors managed to ultimately run only 102 (less than 20%).  These low numbers only reflect the authors' success at running the code and did not verify the validity of the results. 

**Lack of career opportunities: ** Software doesn’t count for career advancement (e.g., promotion and tenure)  in academia, making it an invisible scholarly contribution. Research software is often not cited (31-43%), even in high impact journals [@howison2016software]. Besides the negative impact on career trajectories, this lack of visibility means that incentives to produce sustainable, widely shared, and collaboratively developed software are lacking.  For university staff members, the Research Software Engineer (RSE) movement has begun creating a new class of academic positions that explicitly value software work, but such positions are not very common in the United States.

**Lack of training opportunities:**  When NSF PIs in the BIO directorate were asked about their biggest challenges in leveraging vast amounts of data currently available, lack of training was listed as the single biggest reason [@barone2017unmet]. Although this training deficit describes ability to use existing data science software, the skills needed to develop them are harder to come by [citation]. While programs like Software Carpentry and a handful of university courses offer training in analyzing data, very few train researchers in modern open source software development. This gap remains to be filled.

**Lack of diversity in research software: **Open source communities struggle to gain participation from women and more broadly from underrepresented groups. Diversity of perspectives, ﬁelds, and backgrounds is important in growing a robust research software community. [**TODO**: This is a bare bones placeholder and could use a lot more evidence to make a strong case. I can reach out to some folks for help with citations].


## Valuing producers of research software

Despite the numerous barriers that prevent people from receiving recognition and career success for their research software work, some have successfully overcome them. An exemplar is in this regard is Dr. Fernando Perez, currently an associate professor in statistics at the University of California, Berkeley. For much of his career he worked in a traditional untenured position as a research scientist in neuroscience and computational research, while collaboratively developing an open source notebook interface for the Python programming language as a side project. Over time, his software work started having much more of an impact than any of his traditional scientific contributions. The current evolution of his group’s efforts, the Jupyter ecosystem ([https://jupyter.org/](https://jupyter.org/)), is considered by many to be "universally accepted by the scientific community" and has won him and his team awards such as the [Association for Computer Machinery award for software](https://blog.jupyter.org/jupyter-receives-the-acm-software-system-award-d433b0dfe3a2). More recently, the magnitude of his software contributions and the far reaching impact of this effort ([https://www.theatlantic.com/science/archive/2017/06/gravitational-waves-black-holes/528807/](https://www.theatlantic.com/science/archive/2017/06/gravitational-waves-black-holes/528807/), [https://www.theatlantic.com/science/archive/2018/04/the-scientific-paper-is-obsolete/556676/](https://www.theatlantic.com/science/archive/2018/04/the-scientific-paper-is-obsolete/556676/)) has earned him a fast-tracked tenured position at University of California, Berkeley. This type of unconventional success in a traditional public university is a sign that the recognition of software work as scholarship is changing. While the result of Dr. Perez’ story is promising, it is very difficult to achieve.  Other academics who have produced work of similar or greater magnitude in the past weren’t so lucky.   Travis Oliphant, for example, served as an assistant professor of Electrical and Computer Engineering at Brigham Young University in the early 2000s.  Among his accomplishments during this time, he is credited as the primary creator of NumPy, the Python library for numerical arrays that is the foundation of modern data science tools, and as one of the early contributors to SciPy, the widely used library for scientific Python.  These contributions were deemed insufficient for tenure.  Kirk McKusick, a professor at EECS in Berkeley was denied tenure in the early 1990s because his primary work was on the BSD Software Project, which then went on to become the foundation of the modern internet. [**TODO**: Other examples would be welcome and appreciated. Are there women/people of color who have experienced similar situations that we can highlight here?]

## Why we ran this conceptualization

It comes as no surprise to researchers that software is undervalued in academia. However, substantive evidence supporting this claim is scattered and mostly anecdotal, making it hard to build a convincing case that the research community and their stakeholders need to care more. In 2017 we submitted a proposal to the National Science Foundation’s [Software Infrastructure for Sustained Innovation (S2I2) program](https://www.nsf.gov/pubs/2015/nsf15553/nsf15553.htm) to gather this evidence, identify unique challenges not already being addressed, and to formulate a plan for an institute to implement solutions. The proposal was funded in December 2018, allowing us to engage in various activities over the following 24 months. Despite the awareness of these issues and the existence of similar conceptualizations (albeit domain centric ones), the core challenges around sustainability (of people/software/practices), recognition and credit, and training/workforce development remain poorly understood and poorly addressed.

We used a wide range of approaches to understand the core social and technical challenges of having sustainable research software. These approaches include in-person unconferences, both general and topic focused, an extensive survey, a pilot training event, and a series of ethnographic studies. We mapped out the current state of software related challenges with an extensive survey targeting researchers across the country. We also invited participants to workshops across the country to share critical challenges and brainstorm solutions in small groups. We dug deeper on a couple of core issues that arose repeatedly (credit and incubators) by organizing two focused workshops to tackle them (credit and incubators). This report captures our summary of those workshops, surveys, and studies and describes a core set of activities that would define the work of a future US Research Software Institute.

## What we plan to do and why
 
The conceptualization phase with its workshops, survey, and ethnographic studies elucidated the need in the community for different components of a potential implementation of URSSI. We identified four areas for supporting the research software community - to accelerate science for diverse research domains as well as software engineering as a research area in its own right.

1. Incubator: Sustainability of research software has many aspects beyond good software engineering practices that overlap with diverse areas of expertise. Such areas include technology advice, project management, business planning, usability advice, license management, etc. The incubator service area would focus on providing experts to projects who have a consultancy agreement and could support a project in the different stages of their life cycle – from spinning up a project to first results and uptake of software to planning its sustainability after the funding ends.

2. Education & Training: Many universities offer curricula in software engineering to students but there is a lack of training to meet the community needs to go into depth for different technologies. The survey showed that there is a need to choose a format and timeframe that is suitable for people in the role of Research Software Engineers (RSEs) who have no formal or informal training yet. While The Carpentries, for example, does an excellent job of teaching basic programming and computational & data analytic methods to researchers in a peer-to-peer model, mostly in 2-day courses, URSSI could fill the gap in teaching more in-depth software engineering, software project management, and community development practices in longer engagements, such as a 5-day summer- and/or winter-school. URSSI would collaborate on topics with The Carpentries and the existing software sustainability institutes, such as the Science Gateways Community Institute and the Molecular Sciences Software Institute, so that rather than replicate effort, it identifies the training in the overall research software landscape that is now missing.

3. Policy: Policy is an important area to improve the sustainability of research software. Policies could include guidelines for citation of software, templates for job positions, good software engineering practices as well as bad software engineering practices as counterexamples. The goal would be in collaboration with the international community and initiatives/projects such as US-RSE and the UK SSI to find a common ground on international level while considering the specific situation in the US.

4. Community & Outreach: Many researchers or developers in the role of an RSE are working in silos and the Community & Outreach area of URSSI would connect them to peers and provide access to beneficial material, resources, and contacts to improve this situation. Community engagement would include one-way communication such as a website, blogs and newsletters, two-way communications for discussions such as webinars and online discussion forums as well as face-to-face meetings in the form of workshops. In addition, this area will include a fellows program to support work done by community members that benefits URSSI activities.
 
The impact of building URSSI with these different areas would be to be the focal point for the overall community of software developers as well as for a set of disciplinary communities to accelerate science that needs research software and improve the career paths of those who develop and maintain it, including RSEs. 
